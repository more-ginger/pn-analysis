{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the input-data folder\n",
    "input_data_folder = '../input-data'\n",
    "\n",
    "# List all files in the input-data folder\n",
    "files = os.listdir(input_data_folder)\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each file and read it into a dataframe\n",
    "for file in files:\n",
    "    file_path = os.path.join(input_data_folder, file)\n",
    "    df = pd.read_json(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one unique dataframe\n",
    "full_year = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the unique dataframe\n",
    "full_year.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has two goals. The first is to find what are the most common categories used across news coverage. The second is to find how much overlap there is between categories. Ultimately, the idea is to create a dataset where each category is a node, and based on the amount of overlap between two categories, a corresponding dataset of links where related nodes are more or less distant depending on how many articles these two categories have in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What categories are used the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I clean the data to remove unused features. For this task, I need only:\n",
    "- `_id`, which identifies the unique article in the dataset\n",
    "- `section_name`, which defines the macro-category the article belongs to\n",
    "- `keywords`, a list of words that describe the article\n",
    "- `pub_date`, the publication date of the article\n",
    "- `pub_month`, can be exctracted from `pub_date` and will help in filtering the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the data\n",
    "full_year_essential = full_year[[\"_id\", \"section_name\", \"keywords\", \"pub_date\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the object-like string in keywords to a list\n",
    "full_year_essential['keywords'] = full_year_essential['keywords'].apply(lambda x: [keyword['value'] for keyword in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pub_date string into a valid datetime object\n",
    "full_year_essential[\"pub_date\"] = pd.to_datetime(full_year_essential[\"pub_date\"])\n",
    "# Create a column with only the publication month\n",
    "full_year_essential[\"pub_month\"] = full_year_essential[\"pub_date\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year_essential.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have all the essentials, I need to explode the dataset so that each category becomes a row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another copy of the data\n",
    "exploded_categories = full_year_essential.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_categories = exploded_categories.explode('keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_categories = exploded_categories.rename(columns={\"keywords\": \"keyword\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little test to check whether or not the explode function worked properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to none if the output is truncated\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "article_test = full_year_essential[full_year_essential['_id'] == 'nyt://video/89abe124-169d-590e-8db8-6eaf21316500']\n",
    "print(article_test[\"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_test = exploded_categories[exploded_categories['_id'] == 'nyt://video/89abe124-169d-590e-8db8-6eaf21316500']\n",
    "categories_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = exploded_categories.groupby(['keyword']).size().reset_index(name='count')\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataframe has only two columns. When creating the visualization, the `keyword` column will be the name of our node. The `count`column will become the size of the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.sort_values(by=\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find overlap between categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a list of categories from one article, for example [banana, apple, carrot], I end up with three separate nodes: banana, apple, and carrot. These three nodes are related to each other, because they appear in the same list. By taking each keyword by itself and pairing it with other ones, I am able to find how many times said keywords appear together throughout the dataset. This information allows me to determine how much overlap there is between different categories of articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I iterate over the individual keywords for each article and create a pair of words `(word1, word2)` and count how many times said pairing repeats. Then, I save the result in a dataset that will represent the edges of my network. Each entry has a `source`, `target`, and `strength` property, with `strength` being the frequency of each pairing throughout the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store pair frequencies\n",
    "pair_counts = defaultdict(int)\n",
    "\n",
    "# Iterate over rows to count co-occurrences\n",
    "for keywords in full_year_essential[\"keywords\"]:\n",
    "    for word1, word2 in combinations(sorted(keywords), 2):  # Sort to avoid duplicates (a,b) and (b,a)\n",
    "        pair_counts[(word1, word2)] += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "edges = pd.DataFrame([(k[0], k[1], v) for k, v in pair_counts.items()], columns=[\"source\", \"target\", \"strength\"])\n",
    "\n",
    "print(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.sort_values(by=\"strength\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the nodes and edges datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe there is a smarter way to do this, but basically I just need to export these data to a json with two properties: nodes and edges. This will make it easy to handle the data in D3. First, I export two separate files and then I import them as jsons and merge them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.to_json('../data/temp-nodes.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.to_json('../data/temp-edges.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/temp-nodes.json') as f:\n",
    "    d_nodes = json.load(f)\n",
    "    print(d_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/temp-edges.json') as f:\n",
    "    d_edges = json.load(f)\n",
    "    print(d_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = {}\n",
    "network['nodes'] = d_nodes\n",
    "network['edges'] = d_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/network.json', 'w') as f:\n",
    "    json.dump(network, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
