{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas geonamescache geopy tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find and merge together all instances of places across NYT and Zeit coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook pre-processes the coverage for both newspapers and output a unique csv file where each row is a country. For each country, the number of total articles in the NYT and Zeit coverage is included, as long as the unique identifier for the articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing has some differences based on the news outlet. NYT data already provides information about the nature of the keywords, allowing for an initial grouping of all keywords about geolocations. Conversely, Zeit data only comes with an array of keywords, with no additional information. However, in both cases the approach is similar: once identified keywords that relate to a geolocation, I extract the country for each one of them, then iterate over the original data to find the articles related to a specific location. Ultimately, I group together all locations based on the country, creating a unique array of articles ids, removing their duplicates, and then counting the ids within the array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step, the two datasets are merged together based on the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests_cache\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut,GeocoderUnavailable\n",
    "\n",
    "import geonamescache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year = pd.read_csv(\"../../input-data/temp-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year_essential = full_year[[\"_id\", \"section_name\", \"keywords\", \"pub_date\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year_essential['keywords'] = full_year_essential['keywords'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode keywords in separate rows\n",
    "full_year_essential = full_year_essential.explode(\"keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate columns for keyword type and keyword\n",
    "full_year_essential[\"keyword_type\"] = full_year_essential[\"keywords\"].apply(lambda x: x.get(\"name\") if isinstance(x, dict) else None)\n",
    "full_year_essential[\"keyword\"] = full_year_essential[\"keywords\"].apply(lambda x: x.get(\"value\") if isinstance(x, dict) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year_essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the keywords associated with geolocations\n",
    "locations = full_year_essential[full_year_essential[\"keyword_type\"] == \"glocations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique list of keywords\n",
    "locations = locations[\"keyword\"].unique()\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New df with only one column, the keywords\n",
    "places_df = pd.DataFrame(locations, columns=[\"location\"])\n",
    "places_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of these locations are structured into Place (Country). Here, the part in brackets \n",
    "# is moved to a new column to already have a rough indication of the location country. \n",
    "places_df['country'] = places_df['location'].apply(lambda x: x[x.find(\"(\")+1:x.find(\")\")] if \"(\" in x and \")\" in x else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geolocate individual locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geo_locator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get country\n",
    "def get_country(location):\n",
    "    try:\n",
    "        geo = geolocator.geocode(location, exactly_one=True, language='en', addressdetails=False)\n",
    "        if geo:\n",
    "            return geo.address.split(\",\")[-1].strip()\n",
    "        else:\n",
    "            return \"Not found\"\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Timeout\"\n",
    "    except GeocoderUnavailable:\n",
    "        return \"Unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_df['retrieved_country'] = places_df.apply(lambda x: get_country(x[\"country\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_df.to_csv(\"../../input-data/nyt_retrived_countries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean edge cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start with loading again the dataset, so if we just need to further polish the data we do not need to run the get_country function again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_countries = pd.read_csv(\"../../input-data/nyt_retrived_countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_countries = retrieved_countries.drop(labels=\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking all US states with ambigous 2 letter code (not real ISO code, will default to \"United States\" as country)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_mask = (retrieved_countries['country'].str.len() == 2) | (retrieved_countries['country'].str.len() == 3) | (retrieved_countries['country'] == \"Los Angeles, Calif\") | (retrieved_countries['country'] == \"Miami, Fla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_countries.loc[us_states_mask, \"retrieved_country\"] = \"United States\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking all rows where geopy was not successful at finding a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undefined_mask = (retrieved_countries['retrieved_country'] == \"Not found\") | (retrieved_countries['retrieved_country'] == \"Unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them do not have a match because the US state name is not recognized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_states = ['ALABAMA', 'ALASKA', 'ARIZONA', 'ARKANSAS', 'CALIFORNIA', 'COLORADO', 'CONNECTICUT', 'DELAWARE', 'FLORIDA', 'GEORGIA', 'HAWAII', 'IDAHO', 'ILLINOIS', 'INDIANA', 'IOWA', 'KANSAS', 'KENTUCKY', 'LOUISIANA', 'MAINE', 'MARYLAND', 'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 'MISSISSIPPI', 'MISSOURI', 'MONTANA', 'NEBRASKA', 'NEVADA', 'NEW HAMPSHIRE', 'NEW JERSEY', 'NEW MEXICO', 'NEW YORK', 'NORTH CAROLINA', 'NORTH DAKOTA', 'OHIO', 'OKLAHOMA', 'OREGON', 'PENNSYLVANIA', 'RHODE ISLAND', 'SOUTH CAROLINA', 'SOUTH DAKOTA', 'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT', 'VIRGINIA', 'WASHINGTON', 'WEST VIRGINIA', 'WISCONSIN', 'WYOMING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undefined_countries = retrieved_countries.loc[undefined_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undefined_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undefined_us_states = [country for country in undefined_countries[\"country\"] if country.upper() in list_of_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undefined_us_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_undefined_us = retrieved_countries[\"country\"].isin(undefined_us_states)\n",
    "retrieved_countries.loc[mask_undefined_us, \"retrieved_country\"] = \"United States\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undefined_mask = (retrieved_countries['retrieved_country'] == \"Not found\") | (retrieved_countries['retrieved_country'] == \"Unavailable\")\n",
    "undefined_countries = retrieved_countries.loc[undefined_mask]\n",
    "undefined_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_countries = [\"Armenia\", \"St Vincent\", \"Lebanon\", \"Mali\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_undefined_countries = retrieved_countries[\"country\"].isin(missing_countries)\n",
    "retrieved_countries.loc[mask_undefined_countries, \"retrieved_country\"] = retrieved_countries.loc[mask_undefined_countries, \"country\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siberia_mask = retrieved_countries[\"country\"] == \"Siberia\"\n",
    "retrieved_countries.loc[siberia_mask, \"retrieved_country\"] = \"Russia\"\n",
    "retrieved_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "west_bank_mask = retrieved_countries[\"country\"] == \"West Bank\"\n",
    "retrieved_countries.loc[west_bank_mask, \"retrieved_country\"] = \"Palestinian Territory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_countries = retrieved_countries.drop(retrieved_countries.loc[undefined_mask].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve article ids from original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for index, place in clean_countries.iterrows():\n",
    "    place_mask = full_year_essential[\"keyword\"] == place[\"location\"]\n",
    "    place_coverage = full_year_essential.loc[place_mask, \"_id\"]\n",
    "    data.append((place[\"location\"], place[\"retrieved_country\"], len(place_coverage.values), place_coverage.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_and_ids_df = pd.DataFrame(data, columns=[\"place_keyword\", \"country\", \"count_of_articles\", \"ids_of_articles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_and_ids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_and_ids_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by country and chain ids of articles together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_and_unique_ids = places_and_ids_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "general_countries = (countries_and_unique_ids.groupby('country', as_index=False)['ids_of_articles']\n",
    "         .agg(lambda x: list(chain.from_iterable(x)))\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates from id list and count number of articles for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_countries[\"ids_of_articles\"] = general_countries[\"ids_of_articles\"].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_countries[\"count_of_articles\"] = general_countries[\"ids_of_articles\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve coordinates for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geo_lookup\")\n",
    "session = requests_cache.CachedSession(\"geopy_cache\", expire_after=86400)  # Cache for 1 day\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(location_name, country_code=None):\n",
    "    query = f\"{location_name}, {country_code}\" if country_code else location_name\n",
    "    try:\n",
    "        location = geolocator.geocode(query, timeout=10)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {query}: {e}\")\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_countries[[\"Latitude\", \"Longitude\"]] = general_countries.progress_apply(\n",
    "    lambda row: get_coordinates(row[\"country\"]), axis=1, result_type=\"expand\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_full_year = pd.read_csv(\"../../input-data/zeit-temp-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_full_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create list with all unique keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_full_year[\"keywords\"] = zeit_full_year[\"keywords\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_full_year = zeit_full_year[zeit_full_year['keywords'].notna()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords= list(zeit_full_year[\"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = [\n",
    "    x\n",
    "    for xs in all_keywords\n",
    "    for x in xs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load custom dataset of countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for Germany is a custom list with countries' names in German. The English names for countries and names for cities are pulled from geonamescache. We do not need specific names for cities because the articles tend to include both the German and English name of famous cities (e.g. Rom, Rome) or they always include the country within the list (e.g. Rom, Italien)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: https://www.drupal.org/node/1136336\n",
    "de_countries = pd.read_csv('../../input-data/countries_de.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_de = de_countries[\"Country\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of country and city names from GeoNamesCache\n",
    "gc = geonamescache.GeonamesCache()\n",
    "countries_en = {v['name'] for v in gc.get_countries().values()}\n",
    "cities = {v['name'] for v in gc.get_cities().values()}\n",
    "countries_de = de_countries[\"Country\"].values\n",
    "\n",
    "# Find keywords that match countries (ENG and DE) or cities\n",
    "place_keywords = [word for word in all_keywords if word in countries_en or word in cities or word in countries_de]\n",
    "\n",
    "print(place_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_in_coverage = list(set(place_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations_in_coverage = pd.DataFrame(locations_in_coverage, columns=[\"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations_in_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations_in_coverage[\"country\"] = df_locations_in_coverage.apply(lambda x: get_country(x[\"location\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations_in_coverage = df_locations_in_coverage.sort_values(by=\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations_in_coverage.to_csv(\"../../input-data/zeit-retrieved-countries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine list of retrieved countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of names requires a bit of manual refinement, hence I load a new file for adding the count. Unfortunately, it is difficult to come up with a way to automate the task, because certain keywords are ambigouous (e.g. Gardena, both a city in the US and a company in Germany) and the data coming from the API does not provide clear indication about the nature of keywords. Since the list of names is not super long, it is still possible to consider the special cases one by one. If the dataset will grow, it could be worth to consider an entirely different approach to place extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_countries_refined = pd.read_csv(\"../../input-data/zeit-retrieved-countries-refined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_countries_refined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match locations with article ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_categories = zeit_full_year.explode(\"keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_categories = zeit_categories[[\"uri\", \"keywords\", \"title\", \"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_categories.rename(columns={\"keywords\": \"keyword\", \"uri\": \"_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Exception to include keywords related to Ukraine for Ukraine, like \"Krieg in der Ukraine\" and \"Kiew\"\n",
    "data = []\n",
    "for index, place in zeit_countries_refined.iterrows():\n",
    "    place_mask = zeit_categories[\"keyword\"] == place[\"location\"]\n",
    "    place_coverage = zeit_categories.loc[place_mask, \"_id\"]\n",
    "    data.append((place[\"location\"], place[\"country\"], len(place_coverage.values), place_coverage.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_places_and_ids_df = pd.DataFrame(data, columns=[\"place_keyword\", \"country\", \"count_of_articles\", \"ids_of_articles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_places_and_ids_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by country and chain ids of articles together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_countries_and_unique_ids = zeit_places_and_ids_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "zeit_general_countries = (zeit_countries_and_unique_ids.groupby('country', as_index=False)['ids_of_articles']\n",
    "         .agg(lambda x: list(chain.from_iterable(x)))\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_general_countries[\"ids_of_articles\"] = zeit_general_countries[\"ids_of_articles\"].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_general_countries[\"count_of_articles\"] = zeit_general_countries[\"ids_of_articles\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeit_general_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_locations_coverages = general_countries.merge(zeit_general_countries, how='outer', on=\"country\", suffixes=(\"_nyt\", \"_zeit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorder columns for better readibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_locations_coverages_reordered = joined_locations_coverages[['country', \"Latitude\", \"Longitude\", \"count_of_articles_nyt\", \"count_of_articles_zeit\", \"ids_of_articles_nyt\", \"ids_of_articles_zeit\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_locations_coverages_reordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_locations_coverages_reordered[\"count_of_articles_zeit\"] = joined_locations_coverages_reordered[\"count_of_articles_zeit\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_locations_coverages_reordered[\"count_of_articles_nyt\"]  = joined_locations_coverages_reordered[\"count_of_articles_nyt\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_locations_coverages_reordered['ids_of_articles_nyt'] = joined_locations_coverages_reordered['ids_of_articles_nyt'].apply(lambda d: d if isinstance(d, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_locations_coverages_reordered['ids_of_articles_zeit'] = joined_locations_coverages_reordered['ids_of_articles_zeit'].apply(lambda d: d if isinstance(d, list) else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_locations_coverages_reordered.to_csv(\"../../data/places/coverage_by_country.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
