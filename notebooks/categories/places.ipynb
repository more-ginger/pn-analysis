{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract articles that mention countries, states, or cities as categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pycountry geonamescache geopy requests_cache tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pycountry\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import requests_cache\n",
    "from pandas.core.common import flatten\n",
    "import ast\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all unique instances of places in NYT coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year = pd.read_csv(\"../../input-data/temp-data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year_essential = full_year[[\"_id\", \"section_name\", \"keywords\", \"pub_date\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year_essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year_essential['keywords'] = full_year_essential['keywords'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the object-like string in keywords to a list\n",
    "full_year_essential['keywords'] = full_year_essential['keywords'].apply(lambda x: [keyword['value'] for keyword in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year_essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_keywords = []\n",
    "\n",
    "for article in full_year_essential['keywords']:\n",
    "    for keywords in article:\n",
    "        list_of_keywords.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keywords = list(set(list_of_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage about New York\n",
    "\n",
    "Assuming that everything in the new York section is related to NYC, I create a dataset including only this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_about_ny = full_year_essential[full_year_essential['section_name'] == 'New York']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_about_ny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage about foreign countries and cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary with all possible countries, then filtering the original dataset to keep only the rows where at least one keyword is fund."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dict = {country.name: country.alpha_2 for country in pycountry.countries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_about_abroad = full_year_essential[full_year_essential['keywords'].apply(lambda x: any(keyword in country_dict for keyword in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_about_abroad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage about US States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of US state codes and names, then filter the dataset to keep relevant articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = {country_dict[word] for word in unique_keywords if word in country_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdivision_dict = {subdivision.code for subdivision in pycountry.subdivisions if subdivision.country_code in country_codes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_state_codes = [s.removeprefix(\"US-\") for s in subdivision_dict if 'US-' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_state_names = {state.name: state.code.removeprefix(\"US-\") for state in pycountry.subdivisions if state.country_code == 'US'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_full_names = list(us_state_names.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit helps in disambiguating between different capitalization of states and includes the full name of the state if no code is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\".+ \\(.+?,? ([A-Za-z]{2})\\)$\")\n",
    "places_in_US = []\n",
    "for item in unique_keywords:\n",
    "    match = pattern.match(item)\n",
    "    if item in us_states_full_names:\n",
    "            places_in_US.append(item)\n",
    "    if match:\n",
    "        state = match.group(1)\n",
    "        normalized_state = state.upper() if len(state) == 2 else state  # Convert 2-letter codes to uppercase\n",
    "        if normalized_state in us_state_codes or state in us_state_names:\n",
    "            places_in_US.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_about_US = full_year_essential[full_year_essential['keywords'].apply(lambda x: any(keyword in places_in_US for keyword in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_about_US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the three different datasets and remove possible duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_about_places = pd.concat([coverage_about_ny, coverage_about_abroad, coverage_about_US])\n",
    "coverage_about_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_about_places = coverage_about_places.drop_duplicates(subset='_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_about_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_about_places.to_json(\"../../data/nyt-coverage-places.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What countries are covered the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_place_keywords = []\n",
    "\n",
    "for article in coverage_about_places['keywords']:\n",
    "    for keywords in article:\n",
    "        list_of_place_keywords.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = pd.DataFrame(country_dict.items(), columns=['name', 'code'])\n",
    "countries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_for_places = []\n",
    "for index, row in countries_df.iterrows():\n",
    "    place = row[\"name\"]\n",
    "    if place != \"Georgia\":\n",
    "        matches = [p for p in coverage_about_places[\"keywords\"] if place.lower() in map(str.lower, p)]\n",
    "    else:\n",
    "        matches = []\n",
    "    \n",
    "    matches_for_places.append(len(matches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df[\"count\"] = matches_for_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df.loc[countries_df['name'] == 'United States', 'count'] = len(coverage_about_US)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What US states are covered the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_df = pd.DataFrame(us_state_names.items(), columns=['name', 'code'])\n",
    "us_states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_for_states = []\n",
    "for index, row in us_states_df.iterrows():\n",
    "    place = row[\"name\"]\n",
    "    matches = [p for p in coverage_about_places[\"keywords\"] if place.lower() in map(str.lower, p)]\n",
    "\n",
    "    matches_for_states.append(len(matches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_df[\"count\"] = matches_for_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_coverage_about_places = pd.concat([countries_df, us_states_df])\n",
    "sum_of_coverage_about_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_coverage_about_places.to_json(\"../../data/nyt-sum-places.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are other categories related to places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_related_kws(df):\n",
    "    dictionaries = []\n",
    "    for index, country in df.iterrows():\n",
    "        country_name = country[\"name\"]\n",
    "        country_coverage = coverage_about_places[coverage_about_places[\"keywords\"].apply(lambda x: any(keyword.lower() == country_name.lower() for keyword in x))]\n",
    "        list_of_keywords = list(flatten(country_coverage[\"keywords\"]))\n",
    "        set_of_keywords = set(list_of_keywords)\n",
    "        count_of_kw = []\n",
    "        \n",
    "        for kw in set_of_keywords:\n",
    "            count_of_kw.append(list_of_keywords.count(kw))\n",
    "        \n",
    "        list_of_keywords = list(set(list_of_keywords))\n",
    "        dictionary = dict(zip(list_of_keywords, count_of_kw))\n",
    "        dictionaries.append(dictionary)\n",
    "\n",
    "    df[\"related_keywords\"] = dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_related_kws(us_states_df)\n",
    "count_related_kws(countries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic coordinates for countries and states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geo_lookup\")\n",
    "session = requests_cache.CachedSession(\"geopy_cache\", expire_after=86400)  # Cache for 1 day\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(location_name, country_code=None):\n",
    "    query = f\"{location_name}, {country_code}\" if country_code else location_name\n",
    "    try:\n",
    "        location = geolocator.geocode(query, timeout=10)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {query}: {e}\")\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df[[\"Latitude\", \"Longitude\"]] = countries_df.progress_apply(\n",
    "    lambda row: get_coordinates(row[\"name\"], row[\"code\"]), axis=1, result_type=\"expand\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_df[[\"Latitude\", \"Longitude\"]] = us_states_df.progress_apply(\n",
    "    lambda row: get_coordinates(row[\"name\"], row[\"code\"]), axis=1, result_type=\"expand\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df['Context']='Global'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_df['Context']='Local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_with_categories = pd.concat([countries_df, us_states_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_with_categories.to_json(\"../../data/places/nyt-sum-places.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
